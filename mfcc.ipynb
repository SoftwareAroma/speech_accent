{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import keras\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "\n",
    "tqdm.pandas()\n",
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE_HZ = 16000 # resampling rate in Hz\n",
    "MAX_LENGTH = 128000 # maximum audio interval length to consider (= RATE_HZ * SECONDS)\n",
    "CSV_FILE_PATH: str = \"bio_metadata.csv\"\n",
    "NATIVE_FILE_PATH: str = \"native_bio_metadata.csv\"\n",
    "ALL_SPEAKERS_PATH: str = \"speakers_all.csv\"\n",
    "NON_NATIVE_FILE_PATH: str = \"non_native_bio_metadata.csv\"\n",
    "COL_SIZE: int = 30\n",
    "SILENCE_THRESHOLD: float = .01\n",
    "RATE: int = 2400\n",
    "N_MFCC: int = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract acoustic features from audio files function\n",
    "def extract_mfcc_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=N_MFCC)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    return mfccs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wav from file function\n",
    "def get_wav(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f'./data/audio/{file_name}.wav')\n",
    "        return librosa.core.resample(y=audio, orig_sr=sample_rate, target_sr=RATE, scale=True)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "# convert wave to mfcc function\n",
    "def wave_to_mfcc(audio, sample_rate):\n",
    "    try:\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=N_MFCC)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing audio\")\n",
    "        return None\n",
    "    return mfccs_processed\n",
    "\n",
    "# normalize mfcc function\n",
    "def normalize_mfcc(mfcc):\n",
    "    mms = MinMaxScaler()\n",
    "    return mms.fit_transform(np.abs(mfcc))\n",
    "\n",
    "# to categorical function\n",
    "def to_categorical(y):\n",
    "    lang_dict = {}\n",
    "    for index, language in enumerate(set(y)):\n",
    "        lang_dict[language] = index\n",
    "    y = list(map(lambda x: lang_dict[x],y))\n",
    "    return keras.utils.to_categorical(y, len(lang_dict)), lang_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the native_bio_metadata.csv\n",
    "native_bio_metadata = pd.read_csv(NATIVE_FILE_PATH)\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop href, age_sex, age_of_english_onset, other_languages, birthplace\n",
    "native_bio_metadata.drop(columns=['href', 'age', 'age_of_english_onset', 'other_languages', 'birth_place', 'age_sex', 'length_of_english_residence', 'english_learning_method'], inplace=True)\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the native_bio_metadata\n",
    "native_bio_metadata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \\n and select first one in native_bio_metadata['english_residence']\n",
    "native_bio_metadata['english_residence'] = native_bio_metadata['english_residence'].apply(lambda x: x.split('\\n')[0])\n",
    "native_bio_metadata.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts\n",
    "native_bio_metadata['english_residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_bio_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to categorical function\n",
    "def to_categorical(y):\n",
    "    lang_dict = {}\n",
    "    for index, language in enumerate(set(y)):\n",
    "        lang_dict[language] = index\n",
    "    y = list(map(lambda x: lang_dict[x],y))\n",
    "    return keras.utils.to_categorical(y, len(lang_dict)), lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column 'file' with the full path of the audio file, the audio files are location in './data/native_combined/'\n",
    "native_bio_metadata.loc[:, 'file'] = native_bio_metadata['language_num'].apply(lambda x: f\"data/audio/{x}.wav\")\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_bio_metadata['english_residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_audio(file):\n",
    "    audio,rate = torchaudio.load(str(file))\n",
    "    transform = torchaudio.transforms.Resample(rate,RATE_HZ)\n",
    "    audio = transform(audio).squeeze(0).numpy()\n",
    "    audio = audio[:MAX_LENGTH] # truncate to first part of audio to save RAM\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_bio_metadata['audio'] = native_bio_metadata['file'].progress_apply(get_transform_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_bio_metadata.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an upsampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X = native_bio_metadata.drop(columns=['native_language', 'sex'])\n",
    "y = native_bio_metadata['english_residence']\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled['english_residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = native_bio_metadata['audio'].values\n",
    "y = native_bio_metadata['english_residence'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a counter for train and test labels\n",
    "train_counter = Counter(y_train)\n",
    "test_counter = Counter(y_test)\n",
    "\n",
    "print(f\"Train Count: {train_counter}\")\n",
    "print(f\"Test Count: {test_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat, _ = to_categorical(y_train)\n",
    "y_test_cat, lang_dict = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch model to train the data on\n",
    "class AcousticDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Concatenate the features within each sample\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)  # self.y[idx] is the label index\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AcousticDataset(X_train, y_train_cat)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data loaders\n",
    "val_dataset = AcousticDataset(X_test, y_test_cat)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimension of the input feature\n",
    "input_dim = train_dataset[0][0].shape[0]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimension of the input feature\n",
    "# input_dim = train_dataset[0][0].shape[0] + train_dataset[0][1].shape[0]\n",
    "\n",
    "# input_dim = 42\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 128),  # Change this to match input size 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, len(lang_dict))\n",
    ")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # print(target)\n",
    "        target_indices = torch.argmax(target, dim=1)\n",
    "        # data = data.unsqueeze(1)\n",
    "        # Assuming data has shape (batch_size, 42)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target_indices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for data, target in val_loader:\n",
    "        target_indices = torch.argmax(target, dim=1)\n",
    "        # data = data.unsqueeze(1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target_indices)\n",
    "        val_loss += loss.item() * data.size(0)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation loss\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        target_indices = torch.argmax(target, dim=1)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target_indices).sum().item()\n",
    "        \n",
    "print(f\"Accuracy: {correct / total:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
