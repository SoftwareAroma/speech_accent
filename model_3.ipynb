{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE_HZ = 16000 # resampling rate in Hz\n",
    "MAX_LENGTH = 128000 # maximum audio interval length to consider (= RATE_HZ * SECONDS)\n",
    "CSV_FILE_PATH: str = \"bio_metadata.csv\"\n",
    "NATIVE_FILE_PATH: str = \"native_bio_metadata.csv\"\n",
    "ALL_SPEAKERS_PATH: str = \"speakers_all.csv\"\n",
    "NON_NATIVE_FILE_PATH: str = \"non_native_bio_metadata.csv\"\n",
    "COL_SIZE: int = 30\n",
    "SILENCE_THRESHOLD: float = .01\n",
    "RATE: int = 2400\n",
    "N_MFCC: int = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract acoustic features from audio files function\n",
    "def extract_mfcc_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=N_MFCC)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    return mfccs_processed\n",
    "\n",
    "# extract, chroma_stft, spectral_centroid, \n",
    "# spectral_bandwidth, spectral_rolloff, zero_crossing_rate function\n",
    "def extract_accoustic_features(file_name):\n",
    "    \"\"\"\n",
    "        Extracts accoustic features from audio file\n",
    "        Takes in the file name and returns the following features:\n",
    "        Args:\n",
    "            :param file_name: str: name of the file to extract features from\n",
    "        Returns:\n",
    "            :return mfcc: np.array: Mel-frequency cepstral coefficients\n",
    "            :return chroma_stft: np.array: Chroma short-time Fourier transform\n",
    "            :return spectral_centroid: np.array: Spectral centroid\n",
    "            :return spectral_bandwidth: np.array: Spectral bandwidth\n",
    "            :return spectral_rolloff: np.array: Spectral rolloff\n",
    "            :return zero_crossing_rate: np.array: Zero crossing rate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f'./data/audio/{file_name}')\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=COL_SIZE).T, axis=0)\n",
    "        chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate).T, axis=0)\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate).T, axis=0)\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sample_rate).T, axis=0)\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate).T, axis=0)\n",
    "        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(audio).T, axis=0)\n",
    "        return mfcc, chroma_stft, spectral_centroid, spectral_bandwidth, spectral_rolloff, zero_crossing_rate\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    \n",
    "\n",
    "# extract pitch intensity, duration, loudness, jitter, shimmer, hnr function (prosodic features)\n",
    "def extract_prosodic_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        pitch_intensity = np.mean(librosa.pyin.piptrack(y=audio, sr=sample_rate).T, axis=0)\n",
    "        duration = np.mean(librosa.effects.time_stretch(audio, 1.0).T, axis=0)\n",
    "        loudness = np.mean(librosa.feature.rms(y=audio).T, axis=0)\n",
    "        jitter = np.mean(librosa.effects.jitter(y=audio).T, axis=0)\n",
    "        shimmer = np.mean(librosa.effects.shimmer(y=audio).T, axis=0)\n",
    "        hnr = np.mean(librosa.effects.harmonic(y=audio).T, axis=0)\n",
    "        return pitch_intensity, duration, loudness, jitter, shimmer, hnr\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    \n",
    "\n",
    "# extract plp features function\n",
    "def extract_plp_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        plp = np.mean(librosa.beat.plp(y=audio, sr=sample_rate, n_mfcc=COL_SIZE).T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    return plp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wav from file function\n",
    "def get_wav(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(f'./data/audio/{file_name}.wav')\n",
    "        return librosa.core.resample(y=audio, orig_sr=sample_rate, target_sr=RATE, scale=True)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "# convert wave to mfcc function\n",
    "def wave_to_mfcc(audio, sample_rate):\n",
    "    try:\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=N_MFCC)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing audio\")\n",
    "        return None\n",
    "    return mfccs_processed\n",
    "\n",
    "# normalize mfcc function\n",
    "def normalize_mfcc(mfcc):\n",
    "    mms = MinMaxScaler()\n",
    "    return mms.fit_transform(np.abs(mfcc))\n",
    "\n",
    "# to categorical function\n",
    "def to_categorical(y):\n",
    "    lang_dict = {}\n",
    "    for index, language in enumerate(set(y)):\n",
    "        lang_dict[language] = index\n",
    "    y = list(map(lambda x: lang_dict[x],y))\n",
    "    return keras.utils.to_categorical(y, len(lang_dict)), lang_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the native_bio_metadata.csv\n",
    "native_bio_metadata = pd.read_csv(NATIVE_FILE_PATH)\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop href, age_sex, age_of_english_onset, other_languages, birthplace\n",
    "native_bio_metadata.drop(columns=['href', 'age', 'age_of_english_onset', 'other_languages', 'birth_place', 'age_sex', 'length_of_english_residence', 'english_learning_method'], inplace=True)\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the native_bio_metadata\n",
    "native_bio_metadata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \\n and select first one in native_bio_metadata['english_residence']\n",
    "native_bio_metadata['english_residence'] = native_bio_metadata['english_residence'].apply(lambda x: x.split('\\n')[0])\n",
    "native_bio_metadata.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts\n",
    "native_bio_metadata['english_residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_bio_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to categorical function\n",
    "def to_categorical(y):\n",
    "    lang_dict = {}\n",
    "    for index, language in enumerate(set(y)):\n",
    "        lang_dict[language] = index\n",
    "    y = list(map(lambda x: lang_dict[x],y))\n",
    "    return keras.utils.to_categorical(y, len(lang_dict)), lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column 'file' with the full path of the audio file, the audio files are location in './data/native_combined/'\n",
    "native_bio_metadata.loc[:, 'file'] = native_bio_metadata['language_num'].apply(lambda x: f\"data/audio/{x}.wav\")\n",
    "native_bio_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_bio_metadata['english_residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an upsampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X = native_bio_metadata.drop(columns=['native_language', 'sex'])\n",
    "y = native_bio_metadata['english_residence']\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(Counter(y_resampled).items()))\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled['english_residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_audio(file):\n",
    "    audio,rate = torchaudio.load(str(file))\n",
    "    transform = torchaudio.transforms.Resample(rate,RATE_HZ)\n",
    "    audio = transform(audio).squeeze(0).numpy()\n",
    "    audio = audio[:MAX_LENGTH]\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_resampled['language_num'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_resampled['english_residence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a counter for train and test labels\n",
    "train_counter = Counter(y_train)\n",
    "test_counter = Counter(y_test)\n",
    "\n",
    "print(f\"Train Count: {train_counter}\")\n",
    "print(f\"Test Count: {test_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat, _ = to_categorical(y_train)\n",
    "y_test_cat, lang_dict = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract acoustic features\n",
    "X_train_acoustic = []\n",
    "\n",
    "for audio in X_train:\n",
    "    # audio is a waveform, load it and extract the features\n",
    "    # mfcc, chroma_stft, spectral_centroid, spectral_bandwidth, spectral_rolloff, zero_crossing_rate\n",
    "    mfcc, chroma_stft, _, _, _, _ = extract_accoustic_features(audio + '.wav')\n",
    "    \n",
    "    X_train_acoustic.append([\n",
    "        mfcc,\n",
    "        chroma_stft,\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_acoustic = []\n",
    "\n",
    "for file in X_test:\n",
    "    audio = file\n",
    "    mfcc, chroma_stft, _, _, _, _ = extract_accoustic_features(audio + '.wav')\n",
    "    X_test_acoustic.append([\n",
    "        mfcc,\n",
    "        chroma_stft,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch model to train the data on\n",
    "class AcousticDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Concatenate the features within each sample\n",
    "        features = torch.tensor(np.concatenate(self.X[idx], axis=0), dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)  # self.y[idx] is the label index\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AcousticDataset(X_train_acoustic, y_train_cat)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# create the data loaders\n",
    "val_dataset = AcousticDataset(X_test_acoustic, y_test_cat)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, input_size=42, num_classes=len(lang_dict)):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm1d(256),\n",
    "        )\n",
    "        # Wrap the Linear Blocks\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = x.mean(-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "model = AudioClassifier()\n",
    "summary(model, (42,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, )\n",
    "\n",
    "# adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "n_epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        target_indices = torch.argmax(target, dim=1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target_indices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for data, target in val_loader:\n",
    "        target_indices = torch.argmax(target, dim=1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target_indices)\n",
    "        val_loss += loss.item() * data.size(0)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation loss\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        target_indices = torch.argmax(target, dim=1)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target_indices).sum().item()\n",
    "        \n",
    "print(f\"Accuracy: {correct / total:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
